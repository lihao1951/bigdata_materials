==简要说明一些特征工程方面用到的知识，关于一些原理没有深入讲解和研究，主要是对知识有一个系统性理解，当实际工作中遇到相应问题时可快速解决和分析==

# 特征归一化

## 为什么需要特征归一化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同的指标之间有可比性

我们一般会对数值类的特征进行归一化

在训练中使用归一化，可以更快的找到最优解

## 归一化常用的方法

1. 线性归一化

   $$X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$$

2. 零均值归一化（标准化）

   $$z = \frac{x-\mu}{\sigma}$$

   其中$x$是原始数据，$\mu$是原始数据列的均值，$\sigma$是原始数据列的标准差，即通过转换将原始数据列转换为均值为0标准差为1的分布上

在实际应用中根据需要进行转换，一般来说用标准化的方法较多一些

# 类别类特征转换

## 什么是类别类特征

主要指的是特征取值范围为有限个值，且一般是字符串形式。比如：上几年级，性别，血型等。如果利用决策树等模型，可以直接输入，但是如果使用SVM、LR、神经网络等算法，需要转换为数值型特征

## 如何转换

1. 序号编码

   序号编码一般用在有大小关系的特征上，比如低中高年级，可以将低年级：1，中年级：2，高年级：3，转换之后也保留了大小关系

2. 独热编码（One-Hot Encoding）

   一般用于处理类别间不具备大小关系的特征，比如血型（A、B、AB、O），One-Hot会把这种特征转换为4维的向量，比如A型血-> [1,0,0,0]，O型血->[0,0,0,1]，一般利用One-Hot的方法会采用稀疏化存储向量以节省空间，比如10维的向量->[1,0,1,0,0,0,0,0,0,0]，在稀疏化表示中会如此存储：(10,{0:1,2:1})

3. 二进制编码

   有别于One-Hot编码，对特征的每一个取值进行编码，得到一个二进制表示的编码，有点像霍夫曼编码，可以节省存储空间，比如对血型编码，编码长度为2，那么可以有4种取值方式->[0,0] [0,1] [1,0] [1,1]，这样就分别代表了ABO血型。

# 组合特征的处理

## 什么是组合特征

为了提高模型的拟合能力，通常对于简单的特征进行组合，形成一个复杂的特征表示

## 怎么形成组合特征

可以对两个特征取值两两组合，但是现实的问题是当每个特征的取值非常多时，得到的特征会非常多，那么直接用该特征训练，参数就会非常大。比如有特征如下

| label | 用户id  | 商品id  |
| ----- | ------- | ------- |
| 1     | user001 | good001 |
| 0     | user002 | good002 |
| 1     | user002 | good001 |
| 0     | user003 | good002 |

组合后有

| label | user001-good001 | user002-good001 | user003-good001 | user001-good002 | user002-good002 | user003-good002 |
| ----- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| 1     | 1               | 0               | 0               | 0               | 0               | 0               |
| 0     | 0               | 0               | 0               | 0               | 1               | 0               |
| 1     | 0               | 1               | 0               | 0               | 0               | 0               |
| 0     | 0               | 0               | 0               | 0               | 0               | 1               |

上图种用户有3个，商品有2个，那么得到的特征数个数就为6

由此 用户有m个，商品有n个，那么组合得到的数量就为 m * n，那么如果用LR进行训练，那么参数量也会有m*n个，一般在互联网行业中，用户以百万记，商品也得千百万记，那么得到的参数量比较大，那么就可以对特征进行降维，可以参见矩阵分解等操作

## 有效的特征组合

可以借助梯度提升决策树来做特征的融合

# 文本表示模型

## 词袋模型

Bag of Words

## TF-IDF

词频-逆文档频率

## 主题模型

LSA 、PLSA 、LDA

## 嵌入模型

Word2Vec 、Glove 、Bert等

# 特征选择

当对特征进行转换、组合后，需要对特征进行选择，好处是 选择主要特征，减少模型的复杂度，可以缩短训练时间

一般来说总共有三种方式

1. Filter 过滤

   根据某种规则来对特征进行过滤，筛选出满足要求的特征来形成新的特征集合

   - 方差选择法：计算各个特征的方差，对于不满足阈值的特征进行过滤
   - 相关系数法：先要计算各个特征对于目标的相关系数值，然后进行排序，选择最大的K个特征，一般的相关系数有皮尔逊、斯皮尔曼相关系数，这些系数一般都是指线性关系
   - 卡方检验：也是通过检验定性自变量对定性因变量的相关性
   - 互信息法： 互信息也是评价定性自变量对定性因变量的相关性

2. Wrapper 包装

   根据目标，利用某种模型基于不同的特征子集训练，根据模型的指标来选择特征子集

   递归特征消除法： 主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征选择出来，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。 

3. Embedded 嵌入

   1. 基于正则项L1 L2 来筛选特征，一般来说L1正则项会使得参数系数，那么就会提取出一些代表性的特征
   2. 基于树模型的特征选择，比如GBDT

# 特征降维

## PCA 主成分分析

PCA指的是一套分析方法，我们一般用的是基于协方差计算的PCA，提取出来K个主要特征，可以根据特征的重要程度来选择K的个数

## LDA 线性判别分析

一般来说是对 数据进行低维度投影，使得“类内方差变小，类间方差变大”，即：同一种数据的投影点尽可能的接近，而不同类别数据的类中心之间的距离尽可能的大



上述特征标准化、特征组合、特征选择、特征降维都可见sklearn的feature_selection 、preprocessing 、decomposition discriminant_analysis ，直接利用fit方法调用即可，此处不再强调

# 参考文献

1. [特征工程]( https://www.jianshu.com/p/7066558bd386  "特征工程")
2. [递归特征消除]( https://blog.csdn.net/lijiawei54188/article/details/80584884 "递归特征消除")
3. 百面机器学习





