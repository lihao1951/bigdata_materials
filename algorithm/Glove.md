# Glove词向量

## 引入

学习词向量有两种方法：局部和全局

全局的代表有 LSA PLSA等

局部的代表有 Word2Vec

他们都有各自的缺陷，全局的没有利用好词之间的类比信息，局部的方法没有利用词全局的统计信息

Glove词向量即利用了局部信息也利用了全局的统计信息，全名为：Global Vectors for Word Representation Jeffrey

## 原理

首先引入词汇的共现矩阵$X$,其中每一个元素$X_{ij}$表示词汇$j$出现在词汇$i$的上下文的次数总和，令$X_{i}=\Sigma_{k}X_{ik}$表示所有出现在词汇$i$上下文中的词汇次数综合，$P_{ij}=P(j|i)=X_{ij}/X_i$表示词汇$j$出现在词汇$i$上下文的概率。一般来说，共现矩阵是通过词的窗口计算得到的，比如以下内容，设定窗口大小为3

```txt
I am 30 years old
```

计算次序如下

| 次序 | 中心词 | 窗口内容     |
| ---- | ------ | ------------ |
| 1    | I      | I am         |
| 2    | am     | I am 30      |
| 3    | 30     | am 30 years  |
| 4    | years  | 30 years old |
| 5    | old    | years old    |

通过此种方式计算了$X_{I,am}+=1$,$X_{I,30}+=1,X_{am,30}+=1...$等，这样就得到了共现矩阵。由此得到了词汇的局部信息

然后作者在研究过程中发现了一个有趣的现象，有以下表格表示词汇$i,j,k$之间的关系

| $ratio_{i,j,k}$ | 单词$j，k$相关 | 单词$j,k$不想关 |
| --------------- | -------------- | --------------- |
| 单词$i,j$相关   | 接近于1        | 很大            |
| 单词$i,j$不相关 | 很小           | 接近于1         |

其中：$ratio_{i,j,k}=\frac{P_{i,k}}{P_{j,k}}$，P即为上述描述的方法进行计算

由此现象能说明共现矩阵存在这样的关系，如果我们学习到的词向量也有这种关系，那么就可以说明学习到了共现矩阵的信息，也意味着学习到了全局的信息

那思想有了，怎么得到最后的损失函数呢？

作者给了以下的思路，此方法也是从博客 [理解GloVe模型]( https://blog.csdn.net/u014665013/article/details/79642083 "理解GloVe模型（+总结")拿下来的

- 首先，$\frac{P_{i,k}}{P_{j,k}}=g(v_i,v_j,v_k)$，$g(v_i,v_j,v_k)$是词向量$i,j,k$的计算函数，我们认为等式的左右两边应该是无限接近的，所以$J=\Sigma(\frac{P_{i,j}}{P_{j,k}}-g(v_i,v_j,v_k))^2$，发现J代价函数包含3个词的计算，比较复杂，需要简化

- 要考虑单词$i,j$两者之间的关系，那么我们认为$v_i-v_j$可以合理的考察两个向量间的相似性

- $ratio_{i,j,k}$是一个标量，那么函数$g(v_i,v_j,v_k)$应该最后也是一个标量，那么最后由$v_i-v_j$得到的相减向量后应该乘以一个向量从而得到一个标量，我们可以这么表示$(v_i-v_j)^Tv_k$在$(v_i-v_j)^Tv_k$之上，作者有套用了一层指数运算$exp()$，得到最后的$g(v_i,v_j,v_k)=exp((v_i-v_j)^Tv_k)$，然后公式简化为:$g(v_i,v_j,v_k)=exp((v_i-v_j)^Tv_k)=exp(v_i^Tv_k-v_j^Tv_k)=\frac{exp(v_i^Tv_k)}{exp(v_j^Tv_k)}$

- 上面的式子就变成了$\frac{P_{i,k}}{P_{j,k}}=\frac{exp(v_i^Tv_k)}{exp(v_j^Tv_k)}$,那么直接令$P_{i,k}=exp(v_i^Tv_k),P_{j,k}=exp(v_j^Tv_k)$就可以

- 我们暂时认为$i,j$可互换，那么最后我们只要求解的是$P_{i,k}=exp(v_i^Tv_k)$，两边取对数$log(P_{i,k})=v_i^Tv_k$

- 代价函数就简化为了$J=\Sigma(log(P_{i,k})-v_i^Tv_k)^2$

但上述函数还有一个问题就是一般意义上来说$P_{i,j}!=P(j,i)$，所以需要加上一个补救措施，最终得到代价函数

$$J=\Sigma(-log(P_{i,j}+v_i^Tv_k+b_i+b_j))$$

再考虑权重项，按照 出现频率搞得单词对于权重应该较大的思路，有

$$J=\Sigma f(X_{i,j})(-log(P_{i,j})+v_i^Tv_k+b_i+b_j)$$

$$f(X)=(x/x_{max})^\alpha ,x<x_{max}$$

$$f(X)=1 ,x>=x_{max}$$

一般来说 $\alpha$取0.75

那么以上原理部分介绍完了

## 参考文献

1. [GloVe原理介绍]( https://blog.csdn.net/linchuhai/article/details/97135612 "GloVe原理介绍")

2. [理解GloVe模型](https://blog.csdn.net/u014665013/article/details/79642083 "理解GloVe模型")







